
@thesis{souza_controlling_2015,
	location = {Brazil},
	title = {Controlling the Parallel Execution of Workflows Relying on a Distributed Database},
	url = {https://renan-souza.github.io/files/RenanMScThesis.pdf},
	abstract = {Large-scale computer-based scientific simulations require high performance computing, involve big data manipulation, and are commonly modeled as data-centric scientific workflows managed by a Scientific Workflow Management System ({SWfMS}). In a parallel execution, a {SWfMS} schedules many tasks to the computing resources and Many Task Computing ({MTC}) is the paradigm that contemplates this scenario. In order to manage the execution data necessary for the parallel execution management and tasks' scheduling in {MTC}, an execution engine needs a scalable data structure to accommodate those many tasks. In addition to managing execution data, it has been shown that storing provenance and domain data at runtime enables powerful advantages, such as execution monitoring, discovery of anticipated results, and user steering. Although all these data may be managed using different approaches (e.g., flat log files, {DBMS}, or a hybrid approach), using a centralized {DBMS} has shown to deliver enhanced analytical capabilities at runtime, which is very valuable for end-users. However, if on the one hand using a centralized {DBMS} enables important advantages, on the other hand, it introduces a single point of failure and of contention, which jeopardizes performance in a large scenario. To cope with this, in this work, we propose a novel {SWfMS} architecture that removes the responsibility of a central node to which all other nodes need to communicate for tasks' scheduling, which generates a point of contention; and transfer such responsibility to a distributed {DBMS}. By doing this, we show that our solution frequently attains an efficiency of over 80\% and more than 90\% of gains in relation to an architecture that relies on a centralized {DBMS}, in a 1,000 cores cluster. More importantly, we achieve all these results without abdicating the advantages of using a {DBMS} to manage execution, provenance, and domain data, jointly, at runtime.},
	pagetotal = {97},
	institution = {Federal University of Rio de Janeiro},
	type = {Master Dissertation},
	author = {Souza, Renan and Mattoso, Marta},
	urldate = {2016-10-17},
	date = {2015},
	file = {Renan-Francisco-Santos-Souza-DISSERTACAO-FINAL.pdf:/Users/rfsouza/Library/Application Support/Zotero/Profiles/33j3px8o.default/zotero/storage/EAWSA4S6/Renan-Francisco-Santos-Souza-DISSERTACAO-FINAL.pdf:application/pdf}
}

@inproceedings{barbosa_applying_2016,
	location = {Kuala Lumpur, Malaysia},
	title = {Applying Data Warehousing and Big Data Techniques to Analyze Internet Performance},
	url = {https://renan-souza.github.io/files/NETAPPS2015.pdf},
	abstract = {Measuring the quality of Internet is essential to evaluate the performance of data links around the world and to keep track of how countries have improved their connections throughout the years. Moreover, Internet performance measurements provide understanding for network bottlenecks, trouble-shooting and even insights about the impact of major events such as tsunamis, fiber cuts or social upheavals. For this reason, since 1998, the {PingER} (Ping End-to-end Reporting) initiative at {SLAC} National Accelerator Laboratory monitors end-to-end performance of Internet links spread over 160 countries, providing a worldwide history of Internet performance. Data containing network measurements are daily collected from {PingER} Measurement Agents ({MAs}) and stored into flat files. As a result, {PingER} maintains a valuable fine- grained big dataset consisting of Internet performance data around the world. However, due to the large amounts of data, performing sophisticated joint analyses on those files may be so difficult that it becomes unfeasible in some scenarios. In this paper, we apply data warehousing techniques to transform the data on those flat files into structured data using a data model that facilitates complex analyses. We load the transformed data into a big distributed data warehouse that is able to perform complex analytical queries on large volumes of data in seconds. Finally, we show some data analyses correlating Internet performance data to hypothetical real-world scenarios.},
	booktitle = {Proceedings of the 4th International Conference on Internet Applications, Protocols and Services},
	author = {Barbosa, T. M. S. and Souza, R. and Cruz, S. M. S. and Campos, M. L. M. and Cottrell, Les},
	urldate = {2016-10-17},
	date = {2016},
	file = {NETAPPS2015_submission_52.pdf:/Users/rfsouza/Library/Application Support/Zotero/Profiles/33j3px8o.default/zotero/storage/QPEHH8ND/NETAPPS2015_submission_52.pdf:application/pdf}
}

@inproceedings{souza_monitoramento_2015,
	location = {Recife, Brazil},
	title = {Monitoramento de Desempenho usando Dados de Proveniência e de Domínio durante a Execução de Aplicações Científicas},
	url = {https://renan-souza.github.io/files/WPERFORMANCE15.pdf},
	abstract = {Simulações computacionais, em geral, são compostas pelo encadeamento de aplicações científicas e executadas em ambientes de processamento de alto desempenho. Tais execuções comumente apresentam gargalos associados ao fluxo de dados entre as aplicações. Diversas ferramentas de perfilagem de código têm apoiado a análise de dados de desempenho, como a Tuning and Analysis Utilities ({TAU}). Entretanto, essas ferramentas não favorecem as análises do fluxo de dados. Essas análises podem ser realizadas com a captura de dados de proveniência enriquecidos com dados de domínio extraídos ao longo da execução das simulações. Neste artigo, propomos o monitoramento de dados de desempenho por meio de consultas a uma base de dados de proveniência que integra dados sobre a execução, o fluxo de dados das simulações e os dados de domínio. Mostramos em aplicações científicas como consultas a essa base auxiliam no monitoramento de anomalias no desempenho, em tempo de execução.},
	eventtitle = {{WPerformance}},
	booktitle = {Anais do {XIV} Workshop em Desempenho de Sistemas Computacionais e de Comunicação},
	author = {Souza, Renan and Silva, Vítor and Neves, Leonardo and de Oliveira, Daniel and Mattoso, Marta},
	urldate = {2016-10-17},
	date = {2015},
	file = {WPerformance.vfinal-pos-criticas-v1.pdf:/Users/rfsouza/Library/Application Support/Zotero/Profiles/33j3px8o.default/zotero/storage/S8WBESQR/WPerformance.vfinal-pos-criticas-v1.pdf:application/pdf}
}

@inproceedings{cavalin_building_2016,
	location = {Tomar, Portugal},
	title = {Building a Question-Answering Corpus Using Social Media and News Articles},
	url = {https://renan-souza.github.io/files/PROPOR16.pdf},
	abstract = {Is it possible to develop a reliable {QA}-Corpus using social media data? What are the challenges faced when attempting such a task? In this paper, we discuss these questions and present our findings when developing a {QA}-Corpus on the topic of Brazilian finance. In order to populate our corpus, we relied on opinions from experts on Brazilian finance that are active on the Twitter application. From these experts, we extracted information from news websites that are used as answers in the corpus. Moreover, to effectively provide rankings of answers to questions, we employ novel word vector based similarity measures between short sentences (that accounts for both questions and Tweets). We validated our methods on a recently released dataset of similarity between short Portuguese sentences. Finally, we also discuss the effectiveness of our approach when used to rank answers to questions from real users.},
	pages = {353--358},
	booktitle = {International Conference on Computational Processing of the Portuguese Language},
	publisher = {Springer},
	author = {Cavalin, Paulo and Figueiredo, Flavio and de Bayser, Maíra and Moyano, Luis and Candello, Heloisa and Appel, Ana and Souza, Renan},
	urldate = {2016-10-17},
	date = {2016},
	file = {[PDF] from github.io:/Users/rfsouza/Library/Application Support/Zotero/Profiles/33j3px8o.default/zotero/storage/TSNUPRQA/Cavalin et al. - 2016 - Building a Question-Answering Corpus Using Social .pdf:application/pdf}
}

@article{camata_applying_2016,
	title = {Applying future Exascale {HPC} methodologies in the energy sector},
	url = {https://renan-souza.github.io/files/RussianSuperComputing.pdf},
	abstract = {The appliance of new exascale {HPC} techniques to energy industry simulations is absolutely needed nowadays. In this sense, the common procedure is to customize these techniques to the specific energy sector they are of interest in order to go beyond the state-of-the-art in the required {HPC} exascale simulations. With this aim, the {HPC}4E project is developing new exascale methodologies to three different energy sources that are the present and the future of energy: wind energy production and design, efficient combustion systems for bi- omass-derived fuels (biogas), and exploration geophysics for hydrocarbon reservoirs. In this work, the general exascale advances proposed as part of {HPC}4E and its outcome to specific results in different domains are presented.},
	pages = {9--19},
	author = {Camata, José J. and Cela, José M. and Costa, Danilo and Coutinho, Alvaro L. G. A. and Fernández, Daniel and Jiménez, Carmen and Kourdioumov, Vadim and Mattoso, Marta and Mayo-Garcia, Rafael and Miras, Thomas and Moríñigo, José A. and Navarro, Jorge and de Oliveira, Daniel and Pascoal, Manuel R. and Silva, Vítor and Souza, Renan and Valduriez, Patrick},
	date = {2016},
	keywords = {biomass, exascale, hpc, hydrocarbon, wind energy},
	file = {ArtículoDef_2.pdf:/Users/rfsouza/Library/Application Support/Zotero/Profiles/33j3px8o.default/zotero/storage/GC2NDK69/ArtículoDef_2.pdf:application/pdf}
}

@article{camata_enhancing_2016,
	title = {Enhancing Energy Production with Exascale {HPC} Methods},
	url = {https://renan-souza.github.io/files/ExascaleHPC.pdf},
	abstract = {High Performance Computing ({HPC}) resources have become the key actor for achieving more ambitious challenges in many disciplines. In this step beyond, an explosion on the available parallelism and the use of special purpose processors are crucial. With such a goal, the {HPC}4E project applies new exascale {HPC} techniques to energy industry simulations, customizing them if necessary, and going beyond the state-of-the-art in the required {HPC} exascale simulations for different energy sources. In this paper, a general overview of these methods is presented as well as some specific preliminary results.},
	author = {Camata, José J and Cela, José M and Costa, Danilo and Coutinho, Alvaro L. G. A. and Fernández-Galisteo, Daniel and Jiménez, Carmen and Kourdioumov, Vadim and Mattoso, Marta and Mayo-García, Rafael and Miras, Thomas and Moríñigo, José A and Navarro, Jorge and Navaux, Philippe O A and De Oliveira, Daniel and Rodríguez-Pascual, Manuel and Silva, Vítor and Souza, Renan and Valduriez, Patrick},
	date = {2016},
	file = {Attachment:/Users/rfsouza/Library/Application Support/Zotero/Profiles/33j3px8o.default/zotero/storage/HC3F5PDD/Camata et al. - Unknown - Enhancing Energy Production with Exascale HPC Methods.pdf:application/pdf}
}

@inproceedings{castro_uma_2015,
	location = {Petrópolis, Brazil},
	title = {Uma Abordagem para Publicação de Dados de Proveniência de Workflows Científicos na Web Semântica},
	url = {https://renan-souza.github.io/files/SBBD15-paper.pdf},
	abstract = {A cada ano, cresce o volume de dados de proveniência coletados por meio da execução de simulações computacionais modeladas como workflows científicos. Tais dados auxiliam a reprodutibilidade das simulações e oferecem maior confiabilidade aos resultados. Apesar de essas simulações produzirem dados de proveniência que são enriquecidos com dados de domínio, eles são comumente armazenados em bancos de dados privados aos quais um número restrito de cientistas tem acesso. Isso reduz a capacidade analítica e a possibilidade de se inferir conhecimento a partir dos dados. No entanto, as tecnologias de Web Semântica ({WS}) conseguem facilitar o acesso público a dados na Web de forma estruturada, padronizada, aberta e interoperável. Neste artigo, propomos a adoção de tais tecnologias para publicar dados de proveniência de workflows na {WS}, além de propor uma nova ontologia. A exemplificação da abordagem é baseada em um estudo de caso real na área de bioinformática.},
	eventtitle = {Simpósio Brasileiro de Banco de Dados},
	pages = {111--116},
	booktitle = {Proceedings of the {XXX} Simpósio Brasileiro de Banco de Dados},
	author = {Castro, Rachel and Souza, Renan and Silva, Vitor and Ocaña, Kary and Oliveira, Daniel De and Mattoso, Marta},
	date = {2015},
	file = {artigo-SBBD-v4.2-f.pdf:/Users/rfsouza/Library/Application Support/Zotero/Profiles/33j3px8o.default/zotero/storage/H9X7GFGQ/artigo-SBBD-v4.2-f.pdf:application/pdf}
}

@thesis{miranda_um_2015,
	title = {Um mecanismo de tolerância a falhas em execuções paralelas de workflows apoiadas por banco de dados},
	url = {https://renan-souza.github.io/files/PedroBScThesis.pdf},
	abstract = {Due to the increasing demand for computing power required by scientific workflows, many of them are managed by Scientific Workflow Manage ment Systems ({SWfMSs}) that use parallel techniques in high performance computing environments with a large number of computer nodes. As the number of used computer nodes grows, the probability of at least one of them fail during the scientific workflow running becomes higher. In this scenario, it is essential that {SWfMSs} implement fault tolerance mechanisms. This work proposes a fault tolerance mechanism in {SGWfCs} collecting workflow execution data and storing them in provenance databases at runtime, and implement this mechanism in {SciCumulus}/C² on a Distributed Database System (d-{SCC})},
	pagetotal = {38},
	institution = {Federal University of Rio de Janeiro},
	type = {Bachelor Thesis},
	author = {Miranda, Paiva Pedro and Souza, Renan and Mattoso, Marta},
	date = {2015},
	file = {[PDF] from ufrj.br:/Users/rfsouza/Library/Application Support/Zotero/Profiles/33j3px8o.default/zotero/storage/RU5J58BG/Miranda - 2015 - UM MECANISMO DE TOLERÂNCIA A FALHAS EM EXECUÇÕES P.pdf:application/pdf}
}

@inproceedings{silva_integrating_2016,
	location = {Salt Lake City, {USA}},
	title = {Integrating Domain-data Steering with Code-profiling Tools to Debug Data-intensive Workflows},
	url = {https://renan-souza.github.io/files/WORKS16-2.pdf},
	abstract = {Computer simulations may be composed of several scientific programs chained in a coherent flow and executed in High Performance Computing environments. These executions may present execution anomalies associated to the data that flows in parallel among programs. Several parallel code-profiling tools already support performance analysis, such as Tuning and Analysis Utilities ({TAU}) or provide fine-grain performance statistics such as the System Activity Report ({SAR}). These tools are effective for code profiling, but do not associate their results to dataflow analysis neither are connected to the concept of data-intensive workflows. Such analysis is fundamental and may be performed by capturing execution data enriched with fine-grain domain data during the long- term run of a computer simulation. In this paper, we propose a monitoring data capture approach as a component that couples code- profiling tools to metadata from workflow executions. The goal is to profile and debug parallel executions of workflows through queries to a database that integrates performance, resource consumption, provenance, and domain data from simulation programs flow at runtime. We show how querying this database enables domain-aware runtime steering of performance anomalies by using the astronomy Montage workflow. We show that the overhead introduced by our approach was 0.6\% of the total workflow execution time.},
	booktitle = {12th Workshop on Workflows in Support of Large-Scale Science ({WORKS}) in {ACM}/{IEEE} Supercomputing Workshops},
	author = {Silva, Vítor and Neves, Leonardo and Souza, Renan and Coutinho, Alvaro and Oliveira, Daniel De and Mattoso, Marta},
	date = {2016},
	keywords = {debugging, performance analysis, provenance, scientific workflow},
	file = {WORKS_VFinal.3.pdf:/Users/rfsouza/Library/Application Support/Zotero/Profiles/33j3px8o.default/zotero/storage/DD3E5DN3/WORKS_VFinal.3.pdf:application/pdf}
}

@thesis{souza_processo_2013,
	title = {Processo de Publicação de Dados Abertos Interligados: Aplicação a Dados de Desempenho de Rede},
	url = {https://renan-souza.github.io/files/RenanBScThesis.pdf},
	abstract = {In today's time, in Computer and Information sciences, there is a clear acknowledgement of the semantic web technologies' potential, although it is still noticeable a lack of applications that take advantages of their utilization. The goal of this project is to discuss the usage of these technologies, presenting a proposal of a Linked Open Data ({LOD}) Publishing Process, conceived throughout the development of a solution to publish data about the performance of Internet links around the world, using a standard format on the web ({LOD}). The main characteristics and advantages of having data in this format will be distinctly emphasized in the process, as well as the difficulties faced during the project, highlighting fields of semantic web that still need more research and development.},
	pagetotal = {132},
	institution = {Federal University of Rio de Janeiro},
	type = {Bachelor Thesis},
	author = {Souza, Renan and Campos, M. L. M.},
	date = {2013},
	keywords = {Linked Open Data, Linked Open Data Publication Process, {LOD} Networking Monitoring Measurement, {LOD} Ping Networking, Open Data Standards, {PingER} {LOD}, Semantic Web},
	file = {Attachment:/Users/rfsouza/Library/Application Support/Zotero/Profiles/33j3px8o.default/zotero/storage/BMIX7WNP/FinalV1.2.pdf:application/pdf}
}

@inproceedings{souza_online_2016,
	location = {Salt Lake City, {USA}},
	title = {Online Input Data Reduction in Scientific Workflows},
	url = {https://renan-souza.github.io/files/WORKS16-1.pdf},
	abstract = {Many scientific workflows are data-intensive and need be iteratively executed for large input sets of data elements. Reducing input data is a powerful way to reduce overall execution time in such workflows. When this is accomplished online (i.e., without requiring users to stop execution to reduce the data and resume execution), it can save much time and user interactions can integrate within workflow execution. Then, a major problem is to determine which subset of the input data should be removed. Other related problems include guaranteeing that the workflow system will maintain execution and data consistent after reduction, and keeping track of how users interacted with execution. In this paper, we adopt the approach " human-in-the-loop " for scientific workflows by enabling users to steer the workflow execution and reduce input elements from datasets at runtime. We propose an adaptive monitoring approach that combines workflow provenance monitoring and computational steering to support users in analyzing the evolution of key parameters and determining which subset of the data should be removed. We also extend a provenance data model to keep track of user interactions when users reduce data at runtime. In our experimental validation, we develop a test case from the oil and gas industry, using a 936-cores cluster. The results on our parameter sweep test case show that the user interactions for online data reduction yield a 37\% reduction of execution time. {CCS} Concepts • Massively parallel and high-performance simulations.},
	booktitle = {12th Workshop on Workflows in Support of Large-Scale Science ({WORKS}) in {ACM}/{IEEE} Supercomputing Workshops},
	author = {Souza, Renan and Silva, Vítor and Coutinho, Alvaro L. G. A. and Valduriez, Patrick and Mattoso, Marta},
	date = {2016},
	keywords = {Dynamic Workflows, Human in the Loop, Online Data Reduction, Provenance Data, Scientific Workflows},
	file = {Attachment:/Users/rfsouza/Library/Application Support/Zotero/Profiles/33j3px8o.default/zotero/storage/5QTTCSQ7/Souza et al. - 2016 - Online Input Data Reduction in Scientific Workflows.pdf:application/pdf}
}

@inproceedings{souza_parallel_2015,
	location = {Austin, {USA}},
	title = {Parallel Execution of Workflows Driven by a Distributed Database Management System},
	url_Poster = {https://renan-souza.github.io/files/{SC}15-poster.pdf},
	url = {https://renan-souza.github.io/files/SC15-paper.pdf},
	url_Public = {http://sc15.supercomputing.org/sites/all/themes/{SC}15images/tech\_poster/tech\_poster\_pages/post284.html},
	abstract = {Scientific Workflow Management Systems ({SWfMS}) that execute large-scale simulations need to manage many task computing in high performance environments. With the scale of tasks and processing cores to be managed, {SWfMS} require efficient distributed data structures to manage data related to scheduling, data movement and provenance data gathering. Although related systems store these data in multiple log files, some existing approaches store them using a Database Management System ({DBMS}) at runtime, which provides powerful analytical capabilities, such as execution monitoring, anticipated result analyses, and user steering. Despite these advantages, approaches relying on a centralized {DBMS} introduce a point of contention, jeopardizing performance in large-scale executions. In this paper, we propose an architecture relying on a distributed {DBMS} to both manage the parallel execution of tasks and store those data at runtime. Our experiments show an efficiency of over 80\% on 1,000 cores without abdicating the analytical capabilities at runtime.},
	pages = {2--4},
	booktitle = {{ACM}/{IEEE} Supercomputing},
	author = {Souza, Renan and Silva, Vítor and de Oliveira, Daniel and Valduriez, Patrick and Lima, Alexandre A B and Mattoso, Marta},
	date = {2015},
	file = {d-SCC-v-3.0.pdf:/Users/rfsouza/Library/Application Support/Zotero/Profiles/33j3px8o.default/zotero/storage/CJ35V5NA/d-SCC-v-3.0.pdf:application/pdf;PosterV5.pdf:/Users/rfsouza/Library/Application Support/Zotero/Profiles/33j3px8o.default/zotero/storage/WT4ZV3IT/PosterV5.pdf:application/pdf}
}

@inproceedings{souza_linked_2014,
	location = {Stanford, {USA}},
	title = {Linked open data publication strategies: Application in networking performance measurement data},
	url = {https://renan-souza.github.io/files/ASEBigData2014-paper.pdf},
	url_Public = {Linked open data publication strategies},
	abstract = {Most of the data published on the web is unstructured or does not follow a standard. This makes it harder to retrieve and interchange information between different data sources. This work uses Linked Open Data ({LOD}) technologies and applies them in a scenario that deals with a large amount of computer network measurement data. The goal is to make the data more structured, hence easier to be retrieved, analyzed, and more interoperable. We discuss the challenges of processing large amount of data to: transform it into a standard format ({RDF}); link it to other data sources; and analyze and visualize the transformed data. Moreover, an ontology that aims to minimize the number of triples is proposed and a discussion of how ontologies may impact performance is presented. In addition, both the advantages of having the data in {RDF} format and the obstacles that the {LOD} community still faces are analyzed within the use cases on the scenario of the project.},
	eventtitle = {{ASE} {BIGDATA}/{SOCIALCOM}/{CYBERSECURITY}},
	booktitle = {The Second {ASE} International Conference on Big Data Science and Computing},
	author = {Souza, Renan and Cottrell, Les and White, Bebo and Campos, Maria and Mattoso, Marta},
	urldate = {2016-10-17},
	date = {2014},
	file = {[PDF] from stanford.edu:/Users/rfsouza/Library/Application Support/Zotero/Profiles/33j3px8o.default/zotero/storage/NQABQCFA/Souza et al. - 2014 - Linked open data publication strategies Applicati.pdf:application/pdf;poster-final.pdf:/Users/rfsouza/Library/Application Support/Zotero/Profiles/33j3px8o.default/zotero/storage/PQ7I3NHH/poster-final.pdf:application/pdf}
}

@thesis{castro_publicacao_2015,
	title = {Publicação de Proveniência de Workflows na Web Semântica},
	url = {https://renan-souza.github.io/files/RachelBscThesis.pdf},
	abstract = {Due to the increasing use of scientific workflows to perform simulations that require high computational performance, storing provenance is essential to ensure greater reliability and reproducibility of the experiment executed using a workflow. Although provenance data exist around the world, they are usually not published in a structured or standardized form, which makes scientific data interoperability among different knowledge fields more difficult. Despite being relatively not so popular, the Semantic Web has a notorious potential and is an alternative to enable data publication on the web in a structured, standardized and open way. This project proposes an ontology for a Scientific Workflow Management System ({SWfMS}) and a system that enables the provenance data generated by the {SWfMS} to be selected and published on the Semantic Web.},
	pagetotal = {67},
	institution = {Universidade Federal do Rio de Janeiro},
	type = {Bachelor Thesis},
	author = {Castro, Rachel and Souza, Renan and Mattoso, Marta},
	urldate = {2016-10-17},
	date = {2015},
	file = {[PDF] from ufrj.br:/Users/rfsouza/Library/Application Support/Zotero/Profiles/33j3px8o.default/zotero/storage/FC63RE7K/de Castro - 2015 - PUBLICAÇÃO DE PROVENIÊNCIA DE WORKFLOWS NA WEB SEM.pdf:application/pdf}
}